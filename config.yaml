# Configuration for H100 80GB Training

data:
  urls:
    - "http://images.cocodataset.org/zips/train2017.zip"
    - "http://images.cocodataset.org/zips/val2017.zip"
    - "http://images.cocodataset.org/zips/test2017.zip"

  raw_dir: "data/raw"
  extracted_dir: "data/coco"
  processed_dir: "data/shards"

  samples_per_shard: 10000
  processing_workers: 20  # H100: Use most of 26 vCPUs
  image_size: 256
  rotations: [0, 90, 180, 270]

model:
  name: "apple/mobilevitv2-1.0-imagenet1k-256"
  num_classes: 4
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

training:
  output_dir: "checkpoints"
  logs_dir: "logs"

  batch_size: 2048  # H100: 80GB HBM3
  epochs: 10
  learning_rate: 2.0e-3  # Linear scaling for batch 2048
  weight_decay: 1.0e-4
  gradient_accumulation_steps: 1  # Not needed, batch fits

  patience: 3
  num_workers: 20  # H100: Optimal for 26 vCPUs
  pin_memory: true  # Works great on x86_64
  prefetch_factor: 4  # Aggressive prefetching
  keep_n_checkpoints: 3

export:
  coreml:
    enabled: true
    target_ios: 17
    fp16: true

  onnx:
    enabled: false
    opset_version: 17

logging:
  logs_dir: "logs"