# Configuration for Lambda Cloud H100 PCIe (80GB HBM2e)

data:
  urls:
    - "http://images.cocodataset.org/zips/train2017.zip"
    - "http://images.cocodataset.org/zips/val2017.zip"
    - "http://images.cocodataset.org/zips/test2017.zip"

  raw_dir: "data/raw"
  extracted_dir: "data/coco"
  processed_dir: "data/shards"

  samples_per_shard: 10000
  processing_workers: 20  # Lambda Cloud: 20 of 26 vCPUs
  image_size: 256
  rotations: [0, 90, 180, 270]

model:
  name: "apple/mobilevitv2-1.0-imagenet1k-256"
  num_classes: 4
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

training:
  output_dir: "checkpoints"
  logs_dir: "logs"

  batch_size: 2048  # Lambda H100: 80GB HBM2e
  epochs: 10
  learning_rate: 2.0e-3  # Linear scaling for batch 2048
  weight_decay: 1.0e-4
  gradient_accumulation_steps: 1  # Not needed, batch fits

  patience: 3
  num_workers: 20  # Lambda Cloud: Optimal for 26 vCPUs
  pin_memory: true  # Fast on x86_64
  prefetch_factor: 4  # Leverage 200GiB RAM
  keep_n_checkpoints: 3

export:
  coreml:
    enabled: true
    target_ios: 17
    fp16: true

  onnx:
    enabled: false
    opset_version: 17

logging:
  logs_dir: "logs"